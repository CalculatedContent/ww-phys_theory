\documentclass{article}

%%%%%%%%
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{braket}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[ruled,vlined]{algorithm2e}
\newtheorem{theorem}{Theorem}
\usepackage{physics}



\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\newcommand{\mtx}[1]{\mathbf{#1}}
\newcommand{\m}[1]{\mathbf{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\E}{\mathbb{E}}
\newcommand{\delE}{\nabla_{\theta}\mathbb{E}_{p(x;\theta)}}
\newcommand{\V}{\mathbb{V}_{p(x;\theta)}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\xx}{\frac{\langle x, x' \rangle}{\norm{x}\norm{x'}}}

\newcommand{\dellogp}{\nabla_{\theta}\log p(x;\theta)}
\newcommand{\delp}{\nabla_{\theta}p(x;\theta)}
\newcommand{\p}{p(x;\theta)}
\newcommand{\g}{g(\hat{\epsilon}, \theta)}
\newcommand{\eps}{\epsilon}
\newcommand{\nt}{\nabla_{\theta}}
\newcommand{\kernel}{k(x_i, x_j)}
\newcommand{\ft}{\mathcal{FT}}
\newcommand{\Et}{\mathbb{E}_{\theta}}
\newcommand{\one}{\mathbbm{1}}
\newcommand{\dfz}{\frac{\partial f}{\partial z}}
\newcommand{\det}{\text{det}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand {\nred}[1]{{\color{red}\sf{[#1]}}}



\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
%%%%%%%%%%




% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\newcommand {\john}[1]{{\color{blue}\sf{[john: #1]}}}



\newcommand{\matr}[1]{\mathbf{#1}}

\title{Correlation of the Jacobian and Probability Flow}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  John Y.~Shin \\
  Department of Computer Science\\
  New York University\\
  New York, NY 10033 \\
  \texttt{jys308@nyu.edu} \\
  % examples of more authors
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

Over the past decade, Deep Neural Networks (DNNs) have proven remarkably effective on a wide range of tasks in  computer vision (CV), natural language processing (NLP), as well as other domains. Moreover, larger and deeper DNN models, with hundreds to thousands of layers, perform tasks seemingly impossible just a few years ago.  For example, the CV architecture ResNet \cite{he2016deep} has been successfuly trained with over 1000 layers, showing excellent generalization performance on a wide range of data sets (CIFAR10, CIFAR100, SVHN, ImageNet, etc.). Most recently, OpenAI released the NLP Language model GPT-3 \cite{brown2020language}, which has been trained on nearly a half trillion words, using 175 billion parameters, and achieving state-of-the-art (SOTA) performance on several NLP benchmarks.  

The incredible size and depth of these models poses a new and deep theoretical challenges.
[blah blah blah]
Discuss Energy Landscape and  ruggedly convexity


\nred{What has been done before}

\nred{Cross Sections}
We do have some insight into how the Energy Landscape behaves by visualizing 2-dimensional
cross-sections of small models during training, such as ResNet25.
Maybe can run ourselves ?

\nred{Analysis of the Hessian}
Not really informative.  Hessian only provides local information.

There has been past work on showing that the smoothness of the Hessian as a function of the weights of the final trained model is correlated with good generalization \cite{sagun2017empirical, chaudhari2019entropy}. Recently, there has been work on leveraging stochastic methods for the computation of the empirical spectral density of the Hessian \cite{ghorbani2019investigation, yao2019pyhessian}, and further studies of the spectrum of the Hessian have brought some contention to this claim \cite{goldblum2019truth, maddox2020rethinking, granziol2020flatness}. 

\cite{ghorbani2019investigation, goldblum2019truth, maddox2020rethinking, chaudhari2019entropy, sagun2017empirical, granziol2020flatness, yao2019pyhessian}

\nred{Past Studies of the Jacobian}

The Jacobian of the Neural Network, or the derivative of the Neural Network function with respect to either the data (input/output map) or the weights, has also been studied extensively, covering a wide range of topics, such as it's initialization \cite{pennington2017resurrecting, pennington2018emergence}, as a measure of generalization \cite{sokolic2017robust, novak2018sensitivity, jiang2018predicting, oymak2019generalization, oymak2019generalization, sanyal2019stable} as a way to regularize the network \cite{bagge2018tangent, hoffman2019robust}, it's spectrum \cite{pennington2017resurrecting, pennington2018emergence, ling2019spectrum, zhang2019one, tarnowski2019dynamical, wang2016analysis}, as a learning objective itself \cite{lorrainejacnet}, as a measure of robustness \cite{yu2018interpreting}, as well as it's connections to information geometry \cite{sokol2018information}.


\cite{sokolic2017robust, pennington2017resurrecting, novak2018sensitivity, pennington2018emergence, bagge2018tangent, ling2019spectrum, jiang2018predicting, yu2018interpreting, hoffman2019robust, oymak2019generalization, sokol2018information, lorrainejacnet, oymak2019generalization, zhang2019one, sanyal2019stable, tarnowski2019dynamical, wang2016analysis}


\nred{Empirical Generalization Metrics}
Norm-based metrics such as WeightWatcher
Correlated with generalization / test accuracy.
Best metric is based on power law / heavy tailed.
Not explicitly data dependent



\nred{What needs to be done}
Cross-Section is not a generalization metric, is not global

Importance of unsupervised metrics: self training


In order to characterize the Energy Landscape, traditional approaches attempt to count
the number of local minima (i.e the complexity).  And while this is well for theoretical
analysis (such as spin glass theory, random matrix theory, etc), numerically this
is quite hard.  Especially for the massive production size DNNs in use today.

Here, we suggest an new, alternative approach--to study the Empirical Spectral Density (ESD)
of the data-dependent Jacobian, which is readily calculated with a single epoch of Backprop
using any off-the-shelf toolkit such as TensorFlow, PyTorch, etc. 

Similar to the weightwatcher studies..

Show picture:  compare relatively random / flat vs  a deeply funneled convex Landscape ESDs

random:  real world data, randomly labeled 



Here is a summary of our main results:
\begin{itemize}
\item

\item

\item 

\end{itemize}

\section{Computing the Jacobian}

Suppose we have a Neural Network classifier $f : \R^{d} \rightarrow \R^{k}$, where $d$ is the dimensionality of the input and $k$ is the number of classes. We define the Jacobian matrix of the Neural Network as:

\begin{equation}
J(\matr{x}) = \nabla_\matr{x} f(\matr{x}) \in \R^{k} \times \R^{d}
\end{equation}

Or the derivative of the Neural Network with respect to the input. Suppose we have a training dataset of size $n$, with examples given as $(\matr{x}_1, \matr{x}_2, \cdots, \matr{x}_n)$, where $\matr{x}_i \in \R^d$. We can concatenate the $J(\matr{x})$ for each example into a vector:

\begin{equation}
\mathcal{J}_i = \text{flatten}(J(\matr{x}_i)) \in \R^{k \times d}}
\end{equation}

We can then form a matrix of these $\mathcal{J}_i$'s into a matrix $\mathcal{J} \in \R^n \times \R^{k \times d}$, which we will call the Jacobian matrix of the training dataset. Each row corresponds to a training example and the columns are a concatenation of the derivatives of each dimension of the output with each dimension of the input.

As a classifier trained with a cross-entropy loss and with a softmax as the final output of the network, we can interpret the output of the network as a vector of probabilities:

\begin{equation}
f(\matr{x})  = (p(y_1| \matr{x}), p(y_2| \matr{x}), \cdots, p(y_k| \matr{x})) \in \R^k
\end{equation}

The Jacobian matrix of the neural network then becomes:

\begin{equation}
J(\matr{x}) = \nabla_{\matr{x}} f(\matr{x}) = (\nabla_{\matr{x}} p(y_1| \matr{x}), \nabla_{\matr{x}} p(y_2| x), \cdots, \nabla_{\matr{x}} p(y_k| \matr{x})) \in \R^k \times \R^d 
\end{equation}

For conciseness (and abuse) of notation, we can write the the flattened $\mathcal{J}_i$ in vector notation as:

\begin{equation}
\mathcal{J}_i = \nabla_{\matr{x}_i} p(\matr{y} | \matr{x}_i) \in \R^{k \times d}
\end{equation}

We can then build the correlation matrix of the Jacobian over the training dataset as:

\begin{equation}
M_{ij} = (\mathcal{J} \mathcal{J}^T)_{ij} = \nabla_{\matr{x}_i} p(\matr{y}|\matr{x}_i) \cdot \nabla_{\matr{x}_j} p(\matr{y}|\matr{x}_j) \in \R^{n \times n} 
\end{equation}

\begin{theorem}
testing
\end{theorem}


\bibliographystyle{unsrt}
\bibliography{main.bib}



\end{document}
