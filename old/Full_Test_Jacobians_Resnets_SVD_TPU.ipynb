{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Full-Test-Jacobians-Resnets-SVD-TPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOW+ZlUb/7oBUdiM7iO9WjZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CalculatedContent/ww-phys_theory/blob/master/Full_Test_Jacobians_Resnets_SVD_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YGlUg538lQ4",
        "colab_type": "text"
      },
      "source": [
        "## Notebook for generating Full Jacobian SVD over the test data for ResNet data\n",
        "\n",
        "Can do the full calculation, and a batched (slow, but handles large memory cases)\n",
        "\n",
        "Can handle: ResnetXXX with CIFAR10, CIFAR100, SVHN\n",
        "\n",
        "i.e. CIFAR 10\n",
        "```\n",
        " resnet20_cifar10\n",
        " resnet56_cifar10\n",
        " resnet110_cifar10\n",
        " resnet164bn_cifar10\n",
        " resnet272bn_cifar10\n",
        "\n",
        "full_jacobian()\n",
        "\n",
        "\n",
        " resnet542bn_cifar10\n",
        " resnet1001_cifar10\n",
        " resnet1202_cifar10\n",
        "\n",
        "full_batched_jacobian()\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JBCL3Qj8pl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Jdir =  '/content/drive/My Drive/J_resnets_fullsvd'\n",
        "\n",
        "thismodel = 'resnet1001_cifar100'\n",
        "num_classes= 100\n",
        "num_data = 10000\n",
        "\n",
        "batched = True\n",
        "start_batch = 0\n",
        "batch_size = 50\n",
        "\n",
        "#device = 'cuda:0'\n",
        "#dev = xm.xla_device()\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WoRvkaG6Ea3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "3b546f4f-a0dc-46bd-dc63-4f2ec96c2074"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  4264  100  4264    0     0  78962      0 --:--:-- --:--:-- --:--:-- 78962\n",
            "Updating TPU and VM. This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200515 ...\n",
            "Uninstalling torch-1.6.0a0+bf2bbd9:\n",
            "  Successfully uninstalled torch-1.6.0a0+bf2bbd9\n",
            "Uninstalling torchvision-0.7.0a0+a6073f0:\n",
            "  Successfully uninstalled torchvision-0.7.0a0+a6073f0\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][ 91.0 MiB/ 91.0 MiB]                                                \n",
            "Operation completed over 1 objects/91.0 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][119.5 MiB/119.5 MiB]                                                \n",
            "Operation completed over 1 objects/119.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.3 MiB/  2.3 MiB]                                                \n",
            "Operation completed over 1 objects/2.3 MiB.                                      \n",
            "Processing ./torch-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200515) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200515) (1.18.5)\n",
            "Done updating TPU runtime: <Response [200]>\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.6.0a0+bf2bbd9\n",
            "Processing ./torch_xla-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "  Found existing installation: torch-xla 1.6+2b2085a\n",
            "    Uninstalling torch-xla-1.6+2b2085a:\n",
            "      Successfully uninstalled torch-xla-1.6+2b2085a\n",
            "Successfully installed torch-xla-1.6+2b2085a\n",
            "Processing ./torchvision-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (1.6.0a0+bf2bbd9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200515) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.7.0a0+a6073f0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libomp5 is already the newest version (5.0.1-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 59 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "060zq0_L6a3J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52edf686-f4de-4914-83e5-ce62cbbe2129"
      },
      "source": [
        "# imports pytorch\n",
        "import torch\n",
        "# imports the torch_xla package\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "device = xm.xla_device()\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xla:1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZplJz8WqCkD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a0dc9cf-a046-4995-9bbb-f726e6d3dca9"
      },
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.6/dist-packages (0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQGrAUZLSEy_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "cdb9a58e-e316-482d-9cf0-db25017142ae"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "time: 1.25 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9UHqdnHqCcv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "6f04df98-528c-45ac-a8c5-1f54ccf9819c"
      },
      "source": [
        "!pip install pytorchcv\n",
        "!pip install powerlaw\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorchcv in /usr/local/lib/python3.6/dist-packages (0.0.58)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorchcv) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorchcv) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorchcv) (2020.4.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorchcv) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorchcv) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorchcv) (3.0.4)\n",
            "Requirement already satisfied: powerlaw in /usr/local/lib/python3.6/dist-packages (1.4.6)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.6/dist-packages (from powerlaw) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from powerlaw) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from powerlaw) (3.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from powerlaw) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->powerlaw) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->powerlaw) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->powerlaw) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->powerlaw) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->powerlaw) (1.12.0)\n",
            "time: 5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rBl2Oq6DQu9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "a9244efb-59d3-4cf5-e6d7-9b53867f7db7"
      },
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import math\n",
        "import copy\n",
        "\n",
        "import gc\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('__call__', <function LevelMapper.__call__ at 0x7fcf77f8eb70>), ('__init__', <function LevelMapper.__init__ at 0x7fcf77f8eae8>)]\n",
            "[('__call__', <function BalancedPositiveNegativeSampler.__call__ at 0x7fcf75a4b730>), ('__init__', <function BalancedPositiveNegativeSampler.__init__ at 0x7fcf75a4b6a8>)]\n",
            "[('__init__', <function BoxCoder.__init__ at 0x7fcf75a63c80>), ('decode', <function BoxCoder.decode at 0x7fcf75a63e18>), ('decode_single', <function BoxCoder.decode_single at 0x7fcf75a63ea0>), ('encode', <function BoxCoder.encode at 0x7fcf75a63d08>), ('encode_single', <function BoxCoder.encode_single at 0x7fcf75a63d90>)]\n",
            "[('__call__', <function Matcher.__call__ at 0x7fcf75a4b9d8>), ('__init__', <function Matcher.__init__ at 0x7fcf75a4b8c8>), ('set_low_quality_matches_', <function Matcher.set_low_quality_matches_ at 0x7fcf75a63b70>)]\n",
            "[('__init__', <function ImageList.__init__ at 0x7fcf75a7a048>), ('to', <function ImageList.to at 0x7fcf75a7a0d0>)]\n",
            "[('__init__', <function Timebase.__init__ at 0x7fcf758e60d0>)]\n",
            "[('__init__', <function VideoMetaData.__init__ at 0x7fcf758e6400>)]\n",
            "time: 219 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRm23SSZoRyb",
        "colab_type": "text"
      },
      "source": [
        "### Run InstallPhysTheory notebook "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcXIRwl3NGhs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0557513d-1835-4f12-8ddd-3c2843e63a9f"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/My Drive/Jacobian\")\n",
        "import jacobian as jac\n",
        "import utils"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 7.87 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOROJGbR14rP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c97883d-dd1e-48c8-c51e-f2526e0067c6"
      },
      "source": [
        "import torch\n",
        "from torch.autograd.gradcheck import zero_gradients\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def batch_diagJ(inputs, output):\n",
        "  \"\"\"Computes the diagonal Jacobian by input batches.\n",
        "  Input: Input for the function for which the Jacobian will\n",
        "  computed. It will be batch_size*data_dim. Make sure that the\n",
        "  input is flagged as requires_grad=True with the torch.autograd.Variable\n",
        "  wrapper. \n",
        "  Output: Output of the function for which the Jacobian will\n",
        "  be computed. It will be batch_size*classes\n",
        "  Return: Jacobian of dimension batch_size*classes*data_dim\n",
        "  \"\"\"\n",
        "  assert inputs.requires_grad\n",
        "  print(\"inside batch diagJ\")\n",
        "\n",
        "  num_classes = output.size()[1] #0 index is batch\n",
        "  print(\"num classes\", num_classes)\n",
        "\n",
        "  jacobian = torch.zeros(num_classes, *inputs.size())\n",
        "  grad_output = torch.zeros(*output.size())\n",
        "  print(type(jacobian))\n",
        "  print(device)\n",
        "  grad_output = grad_output.to(device)\n",
        "  jacobian = jacobian.to(device)\n",
        "  print(type(jacobian))\n",
        "\n",
        "  #zero out gradients\n",
        "  #compute gradient for one of the classes\n",
        "  for i in range(num_classes):\n",
        "    print(\"i\", i)\n",
        "\n",
        "    zero_gradients(inputs)\n",
        "    grad_output.zero_()\n",
        "    grad_output[:,i] = 1\n",
        "    output.backward(grad_output, retain_graph=True)\n",
        "    jacobian[i] = inputs.grad.data\n",
        "\n",
        "  return torch.transpose(jacobian, dim0=0, dim1=1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 12.4 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjV56-LApamT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dab76c02-5471-4f02-a7f6-e0500ae1a8d4"
      },
      "source": [
        "def jacobian_batched_full(modelname, model, data_loader, batch_size, start_batch = 0, num_classes=10, device=device, data_dim=3*32*32, formJJ=False, save=True): \n",
        "  '''Compute a full of J^{T}J Jacobian in batch mode.  '''\n",
        "\n",
        "  model.eval() \n",
        "  model = model.to(device)\n",
        "\n",
        "  left_data_loader = data_loader \n",
        "  right_data_loader = copy.deepcopy(data_loader)\n",
        "\n",
        "  JJMat = None \n",
        "  for left_batch, data in enumerate(left_data_loader): \n",
        "    if left_batch < start_batch:\n",
        "      continue\n",
        "    print(left_batch) \n",
        "\n",
        "    inputs, _ = data \n",
        "    inputs = inputs.to(device)\n",
        "    inputs.requires_grad=True\n",
        "    outputs = model(inputs)\n",
        "    J = batch_diagJ(inputs, outputs)\n",
        "    J = J.reshape(batch_size,num_classes*data_dim)\n",
        "\n",
        "    X = J.detach().cpu().numpy()\n",
        "\n",
        "    del data, inputs, outputs, J\n",
        "    gc.collect()\n",
        "    #torch.cuda.empty_cache()\n",
        "    #GPUtil.showUtilization()\n",
        "\n",
        "    JJBlock = None\n",
        "    for right_batch, data in enumerate(right_data_loader):\n",
        "      print(left_batch, right_batch)\n",
        "      inputs, _ = data \n",
        "      inputs = inputs.to(device)\n",
        "      inputs.requires_grad=True\n",
        "      outputs = model(inputs)\n",
        "    \n",
        "      Jt = batch_diagJ(inputs, outputs)\n",
        "      Jt = Jt.reshape(batch_size,num_classes*data_dim)\n",
        "\n",
        "      Xt = Jt.detach().cpu().numpy().transpose()\n",
        "\n",
        "      del data, inputs, outputs, Jt\n",
        "      gc.collect()\n",
        "      #torch.cuda.empty_cache()\n",
        "\n",
        "      block = np.dot(X,Xt)\n",
        "      JJBlock = np.hstack([JJBlock, block]) if JJBlock is not None else block\n",
        "\n",
        "    # end of right_batch loop\n",
        "    \n",
        "    if save:\n",
        "      filename = \"{}/Jfull_{}__{}.csv\".format(Jdir, modelname, left_batch)\n",
        "      np.savetxt(filename, JJBlock, delimiter='\\t')\n",
        "      print(\"Saved file \",filename)\n",
        "\n",
        "    if formJJ:\n",
        "      JJMat = np.vstack([JJMat, JJBlock]) if JJMat is not None else JJBlock\n",
        "\n",
        "      print(\"JJMat shape \", left_batch, JJMat.shape, filename) \n",
        "\n",
        "  # end of left_batch loop\n",
        "\n",
        "  return JJMat\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 40.4 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlaD-n5OsIcO",
        "colab_type": "text"
      },
      "source": [
        "### ResNet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT8SeVRcyv2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aff4d782-a439-4a4a-b363-1c8689fa6462"
      },
      "source": [
        "import pytorchcv\n",
        "from pytorchcv.model_provider import get_model as ptcv_get_model"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 109 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6pvm9Wh1MJz",
        "colab_type": "text"
      },
      "source": [
        "### Compute JJ, in batches or all at once\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPJqJb2yZINy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "464889e2-dd31-4982-a941-86379abfcbe5"
      },
      "source": [
        "def get_datasets(thismodel, batch_size):\n",
        "  print(\"getting dataset for \", thismodel)\n",
        "  if thismodel.endswith('cifar100'):\n",
        "    print(\"loading cifar100\")\n",
        "    train_dataset = datasets.CIFAR100(\n",
        "      root='data', \n",
        "      train=True, \n",
        "      transform=transforms.ToTensor(),\n",
        "      download=True)\n",
        "    test_dataset = datasets.CIFAR100(\n",
        "      root='data', \n",
        "      train=False, \n",
        "      transform=transforms.ToTensor(),\n",
        "      download=True) \n",
        " \n",
        "    train_loader = DataLoader(\n",
        "      dataset=train_dataset, \n",
        "      batch_size=batch_size,\n",
        "      num_workers=4,\n",
        "      shuffle=False)\n",
        "    test_loader = DataLoader(\n",
        "      dataset=test_dataset, \n",
        "      batch_size=batch_size,\n",
        "      num_workers=4,\n",
        "      shuffle=False)\n",
        "\n",
        "  else:\n",
        "    print(\"loading cifar10\")\n",
        "    train_dataset, test_dataset, train_loader, test_loader = utils.get_data(batch_size=batch_size)\n",
        "\n",
        "      \n",
        "  return train_dataset, test_dataset, train_loader, test_loader"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 7.51 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOrW3nihw7aP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "663ef2a7-8ed0-479b-9f28-5450d2c4124a"
      },
      "source": [
        "print(thismodel)\n",
        "train_dataset, test_dataset, train_loader, test_loader = get_datasets(thismodel, batch_size)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "resnet1001_cifar100\n",
            "getting dataset for  resnet1001_cifar100\n",
            "loading cifar100\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "time: 2.04 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZsgnzt1co7q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3e84b1a9-a04e-46be-80e0-c15c9279f0b0"
      },
      "source": [
        "model = ptcv_get_model(thismodel, pretrained=True)  \n",
        "if batched: \n",
        "  jacobian_batched_full(thismodel, model, test_loader, batch_size=batch_size, num_classes=num_classes, start_batch=start_batch)\n",
        "else:\n",
        "  JJMat = jacobian_full(model, test_loader, batch_size=batch_size,  num_classes=num_classes,)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "inside batch diagJ\n",
            "num classes 100\n",
            "<class 'torch.Tensor'>\n",
            "xla:1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHIBjIe-oeso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if batched:\n",
        "  print(\"Reading batched JJBlocks for \", thismodel)\n",
        "  JJMat = None\n",
        "  import os, glob\n",
        "  os.chdir(Jdir)\n",
        "  for filename in glob.glob(\"Jfull_{}*\".format(thismodel)):\n",
        "    print(filename)\n",
        "    JJBlock = np.loadtxt(filename)\n",
        "    JJMat = np.vstack([JJMat, JJBlock]) if JJMat is not None else JJBlock\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSjkSs0fyzou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "40ecae7e-7358-49d5-d8c4-c4d768539355"
      },
      "source": [
        "import torch \n",
        "\n",
        "jacobian = torch.zeros(num_classes, 1000)\n",
        "grad_output = torch.zeros(100)\n",
        "print(type(jacobian))\n",
        "print(device)\n",
        "grad_output = grad_output.to(device)\n",
        "jacobian = jacobian.to(device)\n",
        "print(type(jacobian))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-975f8adfc786>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mjacobian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgrad_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjacobian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_classes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsxEPxFXk_EK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import powerlaw\n",
        "norm = num_classes*3*32*32\n",
        "evals = np.linalg.eigh(JJMat)\n",
        "evals  = (1.0/ norm)*evals\n",
        "\n",
        "fit = powerlaw.Fit(evals, xmin=0.01)\n",
        "alpha = fit.alpha\n",
        "D = fit.D\n",
        "max_eval = np.max(evals)\n",
        "\n",
        "plt.hist(evals, bins=100)\n",
        "plt.title(\"{}  alpha = {:0.2f} max  = {:0.2f} \".format(thismodel, alpha, max_eval))\n",
        "plt.show()\n",
        "\n",
        "print(alpha, D, max_eval)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4edrT68e2MRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}